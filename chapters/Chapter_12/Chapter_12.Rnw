%!Rnw root = ../../Master.Rnw

\chapter{Testing the relationship between  numerical variables}
\label{chap:ch12}

\section{Objective}
By the end of this chapter, you will be able to
\begin{itemize}

\item Interpret  a scatter plot.
\item Interpret the correlation coefficent $r$ and $r^2$.
\item Find and eludicate the regression line and use it to predict values of the response variable.
\item Put into words the concepts of total, unexplained, and explained variation.
\item Use correlation and regression techniques to evaluate two variable relationships.

\end{itemize}



\section{What is Correlation?}

The correlation is a common and useful statistics.  It is a single number from $-1$ to $+1$ that indicates the strength of relationship between the two variables.  If the number is between $-0.3$ and $+0.3 $, then we will say that the relationship is weak or non-existent. Otherwise, we will say that the relationship is either moderate or strong.  By the way, the closer we are to a $-1$ or a $+1$ the stronger the relationship.   The following example shows us how to interpret the results.

\subsection{Example 1}

\begin{minipage}[ht]{7cm}

Let's look at the relationship between the height (in meters) and self esteem (1 - 5 scale where 5 means  higher esteem).  Let's hypothesize that height of  a male  affects the male's self esteem.  We probably don't need to be concerned about the direction of causality, it is  imlikely that self esteem causes a person's height.  We select a random sample of twenty males because we know that male heights are different from female heights.  By not using females heights  will simplify  the analysis.

\end{minipage} \hfill
\begin{minipage}[ht]{6cm}

{\small{
 \begin{tabular}{@{} ccc @{}} \hline % Column formatting, @{} suppresses leading/trailing space
     %  & \multicolumn{3}{c}{Category } \\ \hline 
     % \cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
Person & Height & Self.esteem \\ \hline
1 & 1.73 & 4.11 \\
2 & 1.81 & 4.62 \\
3 & 1.57 & 3.83 \\
4 & 1.91 & 4.44 \\
5	& 1.47 & 3.25 \\
6	& 1.52 & 3.16 \\
7	& 1.71 & 3.87 \\
8	& 1.73 & 4.18 \\
9	& 1.81 & 4.39 \\
10 & 1.75 & 3.70 \\
11 & 1.73 & 3.51 \\
12 & 1.71 & 3.22 \\
13 & 1.61 & 3.73 \\
14 & 1.57 & 3.34 \\
15 & 1.52 & 3.45 \\
16 & 1.61 & 4.06 \\
17 & 1.65 & 4.17 \\
18 & 1.71 & 3.89 \\ 
19 & 1.61 & 3.40 \\
20 & 1.55 &3.61 \\ \hline 
\end{tabular} }}

\end{minipage}

% \subsubsection{Scatter plot}

<<f12_1, echo=FALSE, fig.pos="ht", fig.align="center", fig.width=3.3, fig.height=3.3, fig.cap="Self Esteem by Height">>=
  dt12_1 <- read.csv("data/esteem.csv",header=TRUE)
  plot( dt12_1$Self.esteem ~ dt12_1$Height.m, xlab="Height", ylab="Self Esteem")
  lm_1 <- lm( dt12_1$Self.esteem ~ dt12_1$Height.m )
  abline(lm_1)
  
@

As we review the scatter plot, we see that the direction of the relationship is positive, (i.e., as height increases, self esteem increases).  

Based on the above graphic (Figure: 12.1), there appears to be a relationship between the two variables -- height and self esteem.  But is this relationship real?  We will use the five step model to confirm our initial assumption. 

\begin{itemize}
\item {\bf{Step 1.}}  Making Assumptions and Meeting Test Requirements.

% Requires the booktabs if the memoir class is not being used
\begin{table}[htbp]
   \centering
   
   %\topcaption{Table captions are better up top} % requires the topcapt package
   \begin{tabular}{@{} rl @{}} \hline % Column formatting, @{} suppresses leading/trailing space
      % \multicolumn{2}{c}{Item} \\
      % \cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
      Model: & Random sampling  \\
             & Level of Measurement is interval-ratio \\
             & Bivariate normal distribution \\
             & Linear relationship \\
             & Variance of $Y$ scores is uniform for all $X$ values. \\
             & Sampling distribution is bell-shaped. \\ \hline
             
   \end{tabular}
   \caption{Assumptions }
   \label{tab:t12_1}
\end{table}



\item {\bf{Step 2.}}  Stating the Hypotheses.

 $H_0: \rho = 0$ (there is no relationship between height and self esteem) \\
 $H_a: \rho \neq 0$  ($\rho$ represents the population correlation coefficient.)
 
 \item {\bf{Step 3.}}  Selecting the Sampling Distribution and Establishing the Critical Region.
 
 
 % Requires the booktabs if the memoir class is not being used
\begin{table}[htbp]
   \centering
   
   %\topcaption{Table captions are better up top} % requires the topcapt package
   \begin{tabular}{@{} rcl @{}} \hline % Column formatting, @{} suppresses leading/trailing space
      % \multicolumn{2}{c}{Item} \\
      % \cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
      Sampling Distribution & = & t-distribution \\
      Alpha $(\alpha)$ & = & 0.05 (two-tailed) \\
      Degrees of Freedom  & = & $ n - 2 = 20 - 2 = 18 $  \\
      $t_{critical}$ & = & 2.10 \\ \hline
   \end{tabular}
   \caption{Components of correlation }
   \label{tab:t12_2}
\end{table}

\item {\bf{Step 4.}}  Computing the Test Statistic.

The test statistic is computed by the following equation:

\begin{align*}
t_{obtained} &= r \sqrt{ \frac{n - 2}{1 - r^2}} \\
              &= 0.721 \sqrt{ \frac{20 - 2}{1 - 0.721^2}} \\
              &= 4.4145 \\
\end{align*}

From figure (12.2, SPSS printout), we find Pearson Correlation coefficient, $r = 0.721$.  Using the above equation, the test statistic is 4.41.

  \begin{figure}[ht]
    \centering
    \includegraphics[width=11cm]{chapters/Chapter_12/ext_figure/zCorrA.png} % requires the graphicx package
    \caption{SPSS Correlation Analysis}
    \label{fig:f12_2}
 \end{figure}
  
\item {\bf{Step 5:}} Making a Decision and interpreting the Results of the Test

Since the $t_{obtained}$ (test statistic) is greater than the $t_{critical}$ value ($ 4.41 > 2.10 $), we reject the null hypothesis.  Thus we can say that there is a strong, positive relationship  between male height and male self esteem.

\end{itemize}

 
\pagebreak 

\section{What is Linear Regression?}

Regression analysis is a method that you use when the explanatory and response variables are both numeric, i.e., interval-ratio variables, for example, weights, heights, volumes, and temperature.  The easiest way of knowing if the regression is the appropriate method is to see a scatter plot of the data.  If there is an obvious (linear) relationship between the two variables, then the simple linear regression is the correct method.  Let's examine an example that will help explain the process.

\vspace{5mm}
\begin{minipage}[ht]{7cm}

Let's examine the relationship between the percent change of city population growth  from 2000 and 2008 and the number of annual car thefts.  As a researcher, we review the level of measurement and find that both variables are interval-ratio.  So we consider using the scatter plot.  It has two dimensions, the x-axis which is the explanatory (X) variable and the y-axis which is the response (Y) variable that we want to predict.  

\end{minipage} \hfill
\begin{minipage}[ht]{6cm}

{\small{
   \begin{tabular}{@{} cc @{}} \hline % Column formatting, @{} suppresses leading/trailing space
     %  & \multicolumn{3}{c}{Category } \\ \hline 
     % \cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
     Growth & Car.theft \\ \hline
     9.8 & 118 \\
     5.7 & 204 \\
     1.3 & 294 \\
     2.7 & 163 \\
     26.8 & 764 \\
     3.4 & 95 \\
     16.8 & 392 \\
     11.2 & 581 \\
     8.6 & 601 \\
     2.8 & 144 \\ \hline 
   \end{tabular}
}}
\end{minipage}   

<<f12_3, echo=FALSE, fig.pos="ht", fig.align="center", fig.width=3.5, fig.height=3.5, fig.cap="Car Theft by Growth">>=
  dt12_2 <- read.csv("data/EX12A.csv",header=TRUE)
  plot( dt12_2$Car.theft ~ dt12_2$Growth, xlab="Growth", ylab="Car Theft")
  lm_2 <- lm( dt12_2$Car.theft ~ dt12_2$Growth )
  abline( lm_2 )
  b_1 <- lm_2$coefficient[2]
  b_0 <- lm_2$coefficient[1]
  
  xx_11 <- 20
  xy_11 <- 0
  
  xy_22 <- b_0 + b_1 * 20
  lines(c(xx_11,xy_11), c(xy_22, xy_22), col="red")
  lines(c(xx_11, xx_11), c(xy_11, xy_22), col="red")
  
@

\subsubsection{Interpreting Scatter Plot}

The pattern of points on the graph seem to indicate that  city growth (percent change from 2000 to 2008)  increases so does the number of auto thefts increase.  
To demonstrate this pattern, we can draw a straight line that approximates the best fit among the points (See figure 12.3).

\subsubsection{Does a Relationship Exist?}

The definition of an assocation states that the relationship of two variables will vary in the same direction either positive or negative.  As the \emph{X}-variable increases, the \emph{Y}-variable will increase (decrease if the direction is negative).  
If a relationship exists between these two variables, then the best fitted line through the points will not be parallel with the x-axis.
% In other words, the \emph{X-Y} pair represents a point on the scatter plot.  
In a subsequent section, we will use a statistical method to test this hypothesis.

\subsubsection{How Strong is the association?}

The strength of this relationship is assessed by the density around the fitted regression line.  If the relationship is perfect, then the points of \emph{X - Y} will lie  on the fitted regression line.  When these points are posited away from the line, the relationship become weaker.  What we are really talking about here is the correlation coefficient.

From figure (12.4, SPSS printout), we find Pearson Correlation coefficient, $r = 0.773$ indicating that we have a strong correlation between city growth and car theft.  Note the arrow is pointing to the correlation coefficient.

  \begin{figure}[ht]
    \centering
    \includegraphics[width=11cm]{chapters/Chapter_12/ext_figure/zEX12corr.png} % requires the graphicx package
    \caption{SPSS Correlation Analysis}
    \label{fig:f12_4}
 \end{figure}

\subsubsection{Predicting response variable using the scatter plot}

We can use the scatter plot for prediction.  To show this procedure, let's look at the relationship of city growth and annual car thefts shown in figure 12.3.  Let's say that we want to predict the number of car thefts in a city that has grown by 20 percent from 2000 to 2008.  The predicted score on \emph{Y} (indicated by $\hat{Y}$) to discriminate between the predictions of \emph{Y} from the actual \emph{Y} scores.  First we should locate 20 on the x-axis and then draw a straight line parallel with the y-axis and to the regression line.  From the regression line, draw another line that is parallel to the x-axis to the y-axis.  The predicted \emph{Y} ($\hat{Y}$) is found at the point where the line crosses the y-axis.  We predict that, cities that has grown by 20 percent from 2000 to 2008, the number of car thefts would be around 600 per year.  Later, we will develop an model for making this prediction.

\subsubsection{The Regression Line}

Using the above method is crude because trying to fit a line through a cloud of points by using a free hand.  A mathematical method has been developed and implemented in many statistical programs, such as SPSS, MINITAB, SAS, R, etc., to solve this basic equation:

\begin{equation}
Y = a + bX
\end{equation}

\begin{align*}
Y &= dependentVariable \\
a &= Y-intercept \\
b &= Slope \\
X &= independentVariable \\ 
\end{align*}

\subsubsection{Determining the y-intercept (a)}

From the SPSS output figure 12.5, let's determine the y-intercept of this  problem.

  \begin{figure}[ht]
    \centering
    \includegraphics[width=11cm]{chapters/Chapter_12/ext_figure/zRegD.png} % requires the graphicx package
    \caption{SPSS Regression Analysis}
    \label{fig:f12_5}
 \end{figure}
 
 After reviewing figure 12.5, we find column B and row (Constant) (a) is 139.954.  Our interpretation of the y-intercept is that when \emph{X} = 0, 139.954.  Now keep this number in mind as we determine the slope, because we will use these two numbers to predict the annual car theft based on the percent change that a city experienced from 2000 to 2008. 

\subsubsection{Determine the slope (b) }

From the SPSS output figure 12.5, let's determine the slope of this  problem.


 
After reviewing figure 12.5, we find column B and row Growth that the slope (b) is 22.665.  Our interpretation of the slope is that as the percentage change of the cities growth increases by 1 percent, the number of car thefts increase by 22.665 per year.  Let's evaluate the slope with the five step model. 
 
\begin{itemize}
\item {\bf{Step 1.}}  Making Assumptions and Meeting Test Requirements.

% Requires the booktabs if the memoir class is not being used
\begin{table}[htbp]
   \centering
   
   %\topcaption{Table captions are better up top} % requires the topcapt package
   \begin{tabular}{@{} rl @{}} \hline % Column formatting, @{} suppresses leading/trailing space
      % \multicolumn{2}{c}{Item} \\
      % \cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
      Model: & Random sampling  \\
             & Level of Measurement is interval-ratio \\
             & Bivariate normal distribution \\
             & Linear relationship \\
             & Variance of $Y$ scores is uniform for all $X$ values. \\
             & Sampling distribution is bell-shaped. \\ \hline
             
   \end{tabular}
   \caption{Assumptions }
   \label{tab:t12_3}
\end{table}



\item {\bf{Step 2.}}  Stating the Hypotheses.

 $H_0: \beta = 0$ (there is no relationship between height and self esteem) \\
 $H_a: \beta \neq 0$ (where $\beta$ represents the population slope.)
 
 \item {\bf{Step 3.}}  Selecting the Sampling Distribution and Establishing the Critical Region.
 
 
 % Requires the booktabs if the memoir class is not being used
\begin{table}[htbp]
   \centering
   
   %\topcaption{Table captions are better up top} % requires the topcapt package
   \begin{tabular}{@{} rcl @{}} \hline % Column formatting, @{} suppresses leading/trailing space
      % \multicolumn{2}{c}{Item} \\
      % \cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
      Sampling Distribution & = & t-distribution \\
      Alpha $(\alpha)$ & = & 0.05 (two-tailed) \\
      Degrees of Freedom  & = & $ n - 2 = 10 - 2 = 8 $  \\
      $t_{critical}$ & = & 2.3060 \\ \hline
   \end{tabular}
   \caption{Components of correlation }
   \label{tab:t12_4}
\end{table}

\item {\bf{Step 4.}}  Determine the Test Statistic.

The test statistic is determined from  the following table.



From figure (12.6, SPSS printout), we find the slope (b) = 22.665.  The test statistic $t$ is 3.443, from row Growth and column $t$.


  \begin{figure}[ht]
    \centering
    \includegraphics[width=11cm]{chapters/Chapter_12/ext_figure/zRegD.png} % requires the graphicx package
    \caption{SPSS Regression Analysis}
    \label{fig:f12_6}
 \end{figure}
  
\item {\bf{Step 5:}} Making a Decision and interpreting the Results of the Test

Since the $t_{obtained}$ (test statistic) is greater than the $t_{critical}$ value ($ 3.443 > 2.2060 $), we reject the null hypothesis. Thus we can interpret the slope by saying as the percent change of a city increases by one, the number of annual car thefts increases by 22.7.

\end{itemize} 
 
\subsubsection{Prediction}

Now let's put it all together.  We have an equation.  We will use the same \emph{X}-value = 20 that we used when we used a graphical solution for this problem.

\begin{align*}
\hat{Y} &= a + b(X) \\
CarThefts &= 139.954 + 22.665(Growth) \\
          &= 139.954 + 22.665(20) \\
          &= 593.3 
\end{align*}

We can now say that a city that had percent change growth of 20 percent will have an estimated 593 automobiles stolen in one year.


\subsubsection{Interpreting the Regression}

What variables can she collect to predict the final exam score of an Introductory Statistics course?  Statistics instructors have been asking this question for a long time.  An instructor gathered,  (four explainatory varibles (MT1, MT2, Proj, HW)), data from 64 of her statistics class students and did the following analysis.  

\subsubsection{Descriptive Statistics}

The basic descriptive statistics for the variables are reported in table 12.5.  The average final exam score is 156.1 with a score ranging  from 75 to 191.  The average mid-term 1 exam score is 86.1 with a score ranging from 50 to 99.  As consultants, we should ask ourselves, what explainatory variables should we use to predict the final exam score.   

 % Requires the booktabs if the memoir class is not being used
\begin{table}[htbp]
   \centering
   
   %\topcaption{Table captions are better up top} % requires the topcapt package
   \begin{tabular}{@{} lccccc @{}} \hline % Column formatting, @{} suppresses leading/trailing space
       &  \multicolumn{5}{c}{Descriptive Statistics} \\
      % \cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
      Statistics & Final & MT1 & MT2 & Proj & HW \\ \hline
      Mean & 156.1 & 86.1 & 74.2 & 10.1 & 72.3 \\
      Standard deviation & 24.47 & 11.51 & 18.59 & 1.16 & 11.69 \\
      N & 64 & 64 & 64 & 64 & 64 \\ \hline
   \end{tabular}
   \caption{Descriptive Statistics }
   \label{tab:t12_5}
\end{table}

\begin{itemize}
\item Where 
  \begin{itemize}
  \samepage
  \item Final -- Final exam score
  \samepage
  \item MT1 -- Midterm 1 exam score
  \samepage
  \item MT2 -- Midterm 2 exam score 
  \samepage
  \item Proj -- Project score 
  \samepage
  \item HW -- Homework score
  \samepage
  \end{itemize}
\end{itemize}


\subsubsection{Correlation matrix}

 % Requires the booktabs if the memoir class is not being used
\begin{table}[htbp]
   \centering
   
   \begin{tabular}{@{} lccccc @{}} \hline % Column formatting, @{} suppresses leading/trailing space
      % &  \multicolumn{5}{c}{Pearson Correlation} \\
      % \cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
      Statistics & Final & MT1 & MT2 & Project & HW \\ \hline
      Final & 1  \\
      MT1   & \fbox{0.751} & 1 \\
      MT2   & 0.543 & 0.454 & 1 \\
      Proj & 0.087 & 0.101 & -0.032 & 1 \\
      HW      & 0.273 & 0.301 & 0.218 & 0.093 & 1 \\ \hline
   \end{tabular}
   \caption{Pearson Correlation Coefficent }
   \label{tab:t12_6}
\end{table}

After reviewing the correlation matrix (Table 12.6), we see that at the intersection of Final and MT1 has a correlation coefficient ($r = 0.751$) which is the largest number in the matrix.  This tells us that we have a strong positive relationship between the average final exam score and the average mid-term 1 exam score.  The coefficient of determination ($r^2 = 0.564$) which tells us that the med-term 1 score explains 56.4 percent of the variance in the final exam score.  

\subsubsection{Scatter plot}

The next step in assessing an assocation between two interval-ratio variables is the scatter plot.  Figure 12.7 displays the final exam scores on the vertical, \emph{Y}, axis, and the mid-term 1 exam score on the horizontal, \emph{X}, axis. 

<<f12_7, echo=FALSE, fig.pos="ht", fig.align="center", fig.width=3.5, fig.height=3.5, fig.cap="Mid-term 1 Exam">>=
  dt12_3 <- read.csv("data/grades.csv",header=TRUE)
  plot( dt12_3$Final ~ dt12_3$MT1, xlab="Mid-Term 1 ", ylab="Final")
  lm_3 <- lm( dt12_3$Final ~ dt12_3$MT1 )
  abline( lm_3 )
  b_1 <- lm_3$coefficient[2]
  b_0 <- lm_3$coefficient[1]

@
 
What can we say about the relationship?  The regression line is \emph{not} horizontal, so there is a relationship between these two variables.  The points are scattered around the regression line and we can see that we have a strong positive relationship. 

\subsubsection{Regression Equation}

The next step is to determine the regression equation.  

  \begin{figure}[ht]
    \centering
    \includegraphics[width=12cm]{chapters/Chapter_12/ext_figure/zRegE.png} % requires the graphicx package
    \caption{SPSS Regression Analysis }
    \label{fig:f12_8}
 \end{figure}

After reviewing figure 12.8, we find the \emph{Y}-intercept (\emph{a}) is 18.586, row (constant) and column B.  The slope (\emph{b}) is 1.597, row MT1 and column B.

\begin{align*}
Y &= a + bX \\
Final &= 18.586 + 1.597(MT1)
\end{align*}

Ths interpretation of the slope is as mid-term 1 exam score increase by one unit, the final exam score increase by 1.597 points.

\subsubsection{Diagnostics}

\begin{itemize}
\item There is one other thing that we should look at --  residuals.  
  \begin{itemize}
  \item For each data point, we should have a computed residual:
  
  \begin{equation}
  Residual_i =  observedValue_i - predictedValue_i
  \end{equation}


  \item The histogram of the residuals should be  bell-shaped as in figure 12.9

  
  <<f12_9,  echo=FALSE, fig.pos="ht", fig.align="center", fig.width=3.5, fig.height=3.5, fig.cap="Histogram of Residuals">>=
  hist(lm_3$residuals)
  @

  \item Let's look at figure 12.9.  Here we have a nearly normal-shaped curve can be drawn over the histogram.
  \end{itemize}

\item Another assumption that we need to consider is homoscedasticity, i.e., is the variance of \emph{Y} scores uniform across all values of \emph{X}.

  \begin{itemize}
  
  \item Figure 12.10 shows a pattern of points that is uniform.
  
  <<f12_10, echo=FALSE, fig.pos="ht", fig.align="center", fig.width=3.5, fig.height=3.5, fig.cap="Residual Plot">>=
  
  plot( lm_3$residuals ~ lm_3$fitted.values, xlab="Final -- fitted values ", ylab="Residuals")
  abline( 0,0 )

@

  \end{itemize}
  
\end{itemize}

\subsubsection{Summary}

To conclude this analysis, the linear regression equation, the correlation coefficient, and coefficient of determination suggest that there is a strong, positive relationship between the first mid-term exam score  and the final exam score.  The amount of unexplained variation (total variation (100) - explained variation (55) = 45) suggests that there maybe other variables beside first mid-term exam that have important influence on the final exam score.

We should note one limitation of the simple linear regression.  First correlation is not the same as causation.  In other words, when two variables are highly correlated, this does not necessarily mean that there is a causal relationship.  These results could be taken as support for the proposition that higher mid-term exam scores lead to higher final exam scores, the mere existence of a relationship -- even a strong relationship in the direction predicted by the theory -- does not prove that one variable causes the other.








\section{Key Words}
\begin{itemize}
\item Bivariate normal distribution
\item Coefficient of discrimination
\item Correlation matrix
\item Explained variation
\item Homoscedasticity
\item Linear relationship
\item Pearson's r
\item Regression line
\item Scatter plot
\item Slope
\item Total variation
\item Unexplained variation
\item \emph{Y}-intercept
\item $\hat{Y}$
\end{itemize}
\pagebreak
\section{Exercises}

\begin{enumerate}
\item 
How well does the number of beers a student drinks predict his or her
blood alcohol content? Sixteen student volunteers at The Ohio State University drank a randomly assigned number of cans of
beer. Thirty minutes later, a police officer measured their blood alcohol content (BAC). A scatterplot of the data appears
below.  One student drank 9 beers. You see from the scatterplot that his BAC was about

<<f12_11, echo=FALSE, fig.pos="ht", fig.align="center", fig.width=3.5, fig.height=3.5, fig.cap="Mid-term 1 Exam">>=
  dt12_4 <- read.csv("data/beers.csv",header=TRUE)
  plot( dt12_4$BAC ~ dt12_4$NB, xlab="Number of Beers(NB)", ylab="Blood alcohol content(BAC)")
  lm_4 <- lm( dt12_4$BAC ~ dt12_4$NB )
  abline( lm_4 )
  b_1 <- lm_4$coefficient[2]
  b_0 <- lm_4$coefficient[1]

@

  \begin{enumerate}
  \item 0.019
  \item 0.19
  \item 1.9
  \item 9
  \item 19
  \end{enumerate}

\item 
How well does the number of beers a student drinks predict his or her
blood alcohol content? Sixteen student volunteers at The Ohio State University drank a randomly assigned number of cans of
beer. Thirty minutes later, a police officer measured their blood alcohol content (BAC). A scatterplot of the data appears
below.  The scatterplot shows

<<f12_12, echo=FALSE, fig.pos="ht", fig.align="center", fig.width=3.5, fig.height=3.5, fig.cap="Mid-term 1 Exam">>=
  
  plot( dt12_4$BAC ~ dt12_4$NB, xlab="Number of Beers(NB)", ylab="Blood alcohol content(BAC)")
  lm_4 <- lm( dt12_4$BAC ~ dt12_4$NB )
  abline( lm_4 )
  b_1 <- lm_4$coefficient[2]
  b_0 <- lm_4$coefficient[1]

@

  \begin{enumerate}
  \item a weak negative relationship.
  \item a moderately strong negative relationship.
  \item almost no relationship.
  \item a weak positive relationship.
  \item a moderately strong positive straight-line relationship between number of beers and BAC.
  \end{enumerate}

\item 
How well does the number of beers a student drinks predict his or her
blood alcohol content? Sixteen student volunteers at The Ohio State University drank a randomly assigned number of cans of
beer. Thirty minutes later, a police officer measured their blood alcohol content (BAC). A scatterplot of the data appears
below.  A plausible value of the correlation between number of beers and blood alcohol content, based on the scatterplot, is

<<f12_13, echo=FALSE, fig.pos="ht", fig.align="center", fig.width=3.5, fig.height=3.5, fig.cap="Mid-term 1 Exam">>=
  
  plot( dt12_4$BAC ~ dt12_4$NB, xlab="Number of Beers(NB)", ylab="Blood alcohol content(BAC)")
  lm_4 <- lm( dt12_4$BAC ~ dt12_4$NB )
  abline( lm_4 )
  b_1 <- lm_4$coefficient[2]
  b_0 <- lm_4$coefficient[1]

@

  \begin{enumerate}
  \item  $r = –0.9$.
  \item $r = –0.3$.
  \item $r$ close to 0.
  \item $r = 0.3$.
  \item $r = 0.9$.

  \end{enumerate}

\end{enumerate}
